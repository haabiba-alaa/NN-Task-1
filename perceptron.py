# -*- coding: utf-8 -*-
"""Perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OZDGdA2AQjwlt--Dbl5tOtF8MSchr_96
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

X = df[['body_mass', 'beak_length']].values
y = df['bird category'].values

def train_test_split(X, y, test_size=0.2):
    n_samples = X.shape[0]

    indices = np.random.permutation(n_samples)

    split_index = int(n_samples * (1 - test_size))

    train_indices = indices[:split_index]
    test_indices = indices[split_index:]

    X_train = X[train_indices]
    y_train = y[train_indices]
    X_test = X[test_indices]
    y_test = y[test_indices]

    return (X_train, y_train), (X_test, y_test)

(X_train, y_train), (X_test, y_test) = train_test_split(X, y, test_size=0.2)

print("Training Features:\n", X_train)
print("Training Labels:\n", y_train)
print("Testing Features:\n", X_test)
print("Testing Labels:\n", y_test)

# Define the Perceptron class
class Perceptron:
    def __init__(self, eta=0.01, n_iter=10):
        self.eta = eta
        self.n_iter = n_iter

    def weighted_sum(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def signum(self, v):
        return np.where(v >= 0, 1, -1)

    def predict(self, X):
        return self.signum(self.weighted_sum(X))

    def fit(self, X, y):
        self.w_ = np.zeros(1 + X.shape[1])
        self.errors_ = []

        for _ in range(self.n_iter):
            errors = 0
            for xi, target in zip(X, y):
                v = self.weighted_sum(xi)
                y_pred = self.signum(v)
                update = self.eta * (target - y_pred)
                self.w_[1:] += update * xi
                self.w_[0] += update
                errors += int(update != 0.0)
            self.errors_.append(errors)
        return self


perceptron = Perceptron(eta=0.1, n_iter=10)


y_train = np.where(y_train == 0, -1, 1)
y_test = np.where(y_test == 0, -1, 1)

perceptron.fit(X_train, y_train)



test_preds = perceptron.predict(X_test).astype(int)

correct_predictions = np.sum(y_test == test_preds)
accuracy = correct_predictions / len(y_test) * 100

print("Test Accuracy:", round(accuracy, 2), "%")

def confusion_matrix(y_true, y_pred):
    # Initialize counts
    tp = tn = fp = fn = 0

    # Calculate counts based on predictions
    for true, pred in zip(y_true, y_pred):
        if true == 1 and pred == 1:  # True Positive
            tp += 1
        elif true == -1 and pred == -1:  # True Negative
            tn += 1
        elif true == -1 and pred == 1:  # False Positive
            fp += 1
        elif true == 1 and pred == -1:  # False Negative
            fn += 1

    return np.array([[tn, fp], [fn, tp]])

# Calculate the confusion matrix
cm = confusion_matrix(y_test, test_preds)
print("Confusion Matrix:\n", cm)

# Plotting the confusion matrix
def plot_confusion_matrix(cm):
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()
    tick_marks = np.arange(2)
    plt.xticks(tick_marks, ['Negative', 'Positive'])
    plt.yticks(tick_marks, ['Negative', 'Positive'])

    # Loop over data dimensions and create text annotations.
    thresh = cm.max() / 2.
    for i, j in np.ndindex(cm.shape):
        plt.text(j, i, cm[i, j], horizontalalignment='center',
                 color='white' if cm[i, j] > thresh else 'black')

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

# Call the plotting function
plot_confusion_matrix(cm)
plt.show()